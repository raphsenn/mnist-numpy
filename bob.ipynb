{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('mnist_train.csv')\n",
    "df_test = pd.read_csv('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(df_train.drop(columns=['label'])), np.array(df_train['label'])\n",
    "X_test, y_test = np.array(df_test.drop(columns=['label'])), np.array(df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (60000, 784)\tShape of y_train: (60000,)\n",
      "Shape of X_test:  (10000, 784)\tShape of y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of X_train: {X_train.shape}\\tShape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test:  {X_test.shape}\\tShape of y_test:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def relu(z: np.ndarray, derv: bool=False) -> np.ndarray:\n",
    "    if derv: return np.where(z > 0, 1, 0) \n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "\n",
    "def softmax(z: np.ndarray, derv: bool=False) -> np.ndarray:\n",
    "    if derv:\n",
    "        return np.array([(np.exp(z[i]) * (np.sum(np.exp(z)) - 1)) / np.sum(np.exp(z))**2 for i in range(len(z))]) \n",
    "    return np.array([np.exp(z[i])/np.sum(np.exp(z)) for i in range(len(z))]) \n",
    "\n",
    "\n",
    "def one_hot(y: np.ndarray, num_classes: int=10) -> np.ndarray:\n",
    "    y = y.reshape(-1)\n",
    "    y_hot = np.eye(num_classes)[y]\n",
    "    return y_hot\n",
    "\n",
    "\n",
    "class BobNet:\n",
    "    def __init__(self, n_in: int, n_hidden: int, n_out: int) -> None:\n",
    "        self.w1 = np.random.rand(n_in, n_hidden)\n",
    "        self.b1 = np.random.rand(n_hidden)\n",
    "        self.w2 = np.random.rand(n_hidden, n_out)\n",
    "        self.b2 = np.random.rand(n_out)\n",
    "\n",
    "    def fit(self,\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            lr: float=0.1,\n",
    "            epochs: int=100, \n",
    "            batch_size: int=16,\n",
    "            verbose: bool=True) -> None:\n",
    "        N = X.shape[0] \n",
    "\n",
    "        # Simple implementation of stochastic gradient descent. \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle dataset\n",
    "            indices = np.random.permutation(N)\n",
    "            X, y = X[indices], y[indices]    \n",
    "            y_hot = one_hot(y)\n",
    "\n",
    "            # Iterate over mini-batches \n",
    "            for i in range(0, N, batch_size):\n",
    "                X_batch, y_batch = X[i:i+batch_size], y[i:i+batch_size]\n",
    "                y_batch_hot = one_hot(y_batch, num_classes=self.w2.shape[1])\n",
    "\n",
    "\n",
    "                # Forward pass\n",
    "                z1 = np.dot(X_batch, self.w1) + self.b1                     # N x 512\n",
    "                h1 = relu(z1)                                               # N x 512\n",
    "                z2 = np.dot(h1, self.w2) + self.b2                          # N x 10\n",
    "                h2 = softmax(z2)                                            # N x 10\n",
    "\n",
    "                # Backpropagation\n",
    "\n",
    "                # dh2 = -(h2 - y_batch_hot)/batch_size                         # N x 10\n",
    "                # dz2 = dh2 * softmax(z2, derv=True)                          # N x 10\n",
    "                dz2 = (h2 - y_batch_hot)/batch_size \n",
    "                dw2 = np.dot(h1.T, dz2)                                     # 512 x 10\n",
    "                db2 = np.sum(dz2, axis=0)\n",
    "\n",
    "                dh1 = np.dot(dz2, self.w2.T)                                # N x 512\n",
    "                dz1 = dh1 * relu(z1, derv=True)                             # N x 512\n",
    "                dw1 = np.dot(X_batch.T, dh1)                                # 784 x 512\n",
    "                db1 = np.sum(dz1, axis=0)\n",
    "\n",
    "                self.w2 = self.w2 - lr * dw2\n",
    "                self.b2 = self.b2 - lr * db2\n",
    "                self.w1 = self.w1 - lr * dw1\n",
    "                self.b1 = self.b1 - lr * db1\n",
    "\n",
    "            #if verbose:\n",
    "            #    y_hat = self.predict(X)\n",
    "                # y_hat_hot = one_hot(y_hat)\n",
    "                # cross_entropy_loss = - np.sum(y_hot * np.log(y_hat))\n",
    "            #    pred = np.argmax(y_hat, 1)\n",
    "            #    acc = (pred == y).mean()\n",
    "             #   print(f'Acc: {acc}')\n",
    "\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        x = np.dot(x, self.w1) + self.b1\n",
    "        x = relu(x)\n",
    "        x = np.dot(x, self.w2) + self.b2\n",
    "        return softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/8txvxcxn749f_382tm389wwm0000gn/T/ipykernel_24959/4183628589.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return np.array([np.exp(z[i])/np.sum(np.exp(z)) for i in range(len(z))])\n",
      "/var/folders/63/8txvxcxn749f_382tm389wwm0000gn/T/ipykernel_24959/4183628589.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.array([np.exp(z[i])/np.sum(np.exp(z)) for i in range(len(z))])\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network, train and predict\n",
    "bobnet = BobNet(n_in=784, n_hidden=512, n_out=10)\n",
    "bobnet.fit(X_train, y_train, lr=0.00000000001, epochs=1)\n",
    "y_hat = bobnet.predict(X_test)\n",
    "y_hat = np.argmax(y_hat, 1)\n",
    "accuracy = (y_hat == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.098\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0.2, 0.4, 0.2, 0.2], [0.1, 0.1, 0.1, 0.6]])\n",
    "print(np.argmax(y, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
